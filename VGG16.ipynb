{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T12:26:52.123972Z",
     "iopub.status.busy": "2021-12-21T12:26:52.123674Z",
     "iopub.status.idle": "2021-12-21T12:26:52.900360Z",
     "shell.execute_reply": "2021-12-21T12:26:52.899639Z",
     "shell.execute_reply.started": "2021-12-21T12:26:52.123890Z"
    }
   },
   "outputs": [],
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as tnn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "RANDOM_SEED = 90\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T12:26:52.903433Z",
     "iopub.status.busy": "2021-12-21T12:26:52.903177Z",
     "iopub.status.idle": "2021-12-21T12:26:52.908672Z",
     "shell.execute_reply": "2021-12-21T12:26:52.907958Z",
     "shell.execute_reply.started": "2021-12-21T12:26:52.903403Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "#     transforms.RandomResizedCrop(224),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n",
    "                         std  = [ 0.229, 0.224, 0.225 ]),\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T12:26:52.910612Z",
     "iopub.status.busy": "2021-12-21T12:26:52.910111Z",
     "iopub.status.idle": "2021-12-21T12:26:53.545726Z",
     "shell.execute_reply": "2021-12-21T12:26:53.545013Z",
     "shell.execute_reply.started": "2021-12-21T12:26:52.910571Z"
    }
   },
   "outputs": [],
   "source": [
    "trainData = dsets.ImageFolder('/kaggle/input/fruits/fruits-360_dataset/fruits-360/Training/',transform=transform)\n",
    "testData = dsets.ImageFolder('/kaggle/input/fruits/fruits-360_dataset/fruits-360/Test/', transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T12:26:53.549179Z",
     "iopub.status.busy": "2021-12-21T12:26:53.548910Z",
     "iopub.status.idle": "2021-12-21T12:26:53.553459Z",
     "shell.execute_reply": "2021-12-21T12:26:53.552490Z",
     "shell.execute_reply.started": "2021-12-21T12:26:53.549143Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T12:26:53.555481Z",
     "iopub.status.busy": "2021-12-21T12:26:53.554934Z",
     "iopub.status.idle": "2021-12-21T12:26:53.582795Z",
     "shell.execute_reply": "2021-12-21T12:26:53.582051Z",
     "shell.execute_reply.started": "2021-12-21T12:26:53.555444Z"
    }
   },
   "outputs": [],
   "source": [
    "class VGG16(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block_1 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=3,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(64),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Conv2d(in_channels=64,\n",
    "                                out_channels=64,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(64),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                   stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_2 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=64,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Conv2d(in_channels=128,\n",
    "                                out_channels=128,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(128),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                   stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_3 = torch.nn.Sequential(        \n",
    "                torch.nn.Conv2d(in_channels=128,\n",
    "                                out_channels=256,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Conv2d(in_channels=256,\n",
    "                                out_channels=256,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(256),\n",
    "                torch.nn.ReLU(),        \n",
    "                torch.nn.Conv2d(in_channels=256,\n",
    "                                out_channels=256,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(256),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                   stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "          \n",
    "        self.block_4 = torch.nn.Sequential(   \n",
    "                torch.nn.Conv2d(in_channels=256,\n",
    "                                out_channels=512,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(512),\n",
    "                torch.nn.ReLU(),        \n",
    "                torch.nn.Conv2d(in_channels=512,\n",
    "                                out_channels=512,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(512),\n",
    "                torch.nn.ReLU(),        \n",
    "                torch.nn.Conv2d(in_channels=512,\n",
    "                                out_channels=512,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(512),\n",
    "                torch.nn.ReLU(),            \n",
    "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                   stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_5 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=512,\n",
    "                                out_channels=512,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(512),\n",
    "                torch.nn.ReLU(),            \n",
    "                torch.nn.Conv2d(in_channels=512,\n",
    "                                out_channels=512,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(512),\n",
    "                torch.nn.ReLU(),            \n",
    "                torch.nn.Conv2d(in_channels=512,\n",
    "                                out_channels=512,\n",
    "                                kernel_size=(3, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=1),\n",
    "                torch.nn.BatchNorm2d(512),\n",
    "                torch.nn.ReLU(),    \n",
    "                torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                   stride=(2, 2))             \n",
    "        )\n",
    "            \n",
    "        height, width = 3, 3 ## you may want to change that depending on the input image size\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(512*7*7, 4096),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.Linear(4096, 4096),\n",
    "            torch.nn.ReLU(True),\n",
    "            torch.nn.Dropout(p=0.5),\n",
    "            torch.nn.Linear(4096, num_classes),\n",
    "        )\n",
    "            \n",
    "        \n",
    "                    \n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d((height, width))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "#         x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        #probas = F.softmax(logits, dim=1)\n",
    "\n",
    "        return logits                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T12:26:53.584529Z",
     "iopub.status.busy": "2021-12-21T12:26:53.584273Z",
     "iopub.status.idle": "2021-12-21T12:26:56.718395Z",
     "shell.execute_reply": "2021-12-21T12:26:56.717609Z",
     "shell.execute_reply.started": "2021-12-21T12:26:53.584494Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VGG16(num_classes=131)\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), momentum=0.9, lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 7, gamma=0.1)\n",
    "\n",
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T12:26:56.720050Z",
     "iopub.status.busy": "2021-12-21T12:26:56.719799Z",
     "iopub.status.idle": "2021-12-21T13:16:15.753047Z",
     "shell.execute_reply": "2021-12-21T13:16:15.752311Z",
     "shell.execute_reply.started": "2021-12-21T12:26:56.720015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [20/2116], Loss: 5.4518\n",
      "Epoch [1/5], Step [40/2116], Loss: 4.8317\n",
      "Epoch [1/5], Step [60/2116], Loss: 4.3358\n",
      "Epoch [1/5], Step [80/2116], Loss: 4.3052\n",
      "Epoch [1/5], Step [100/2116], Loss: 3.8891\n",
      "Epoch [1/5], Step [120/2116], Loss: 3.7722\n",
      "Epoch [1/5], Step [140/2116], Loss: 3.6538\n",
      "Epoch [1/5], Step [160/2116], Loss: 3.8110\n",
      "Epoch [1/5], Step [180/2116], Loss: 3.5585\n",
      "Epoch [1/5], Step [200/2116], Loss: 3.4799\n",
      "Epoch [1/5], Step [220/2116], Loss: 2.7094\n",
      "Epoch [1/5], Step [240/2116], Loss: 2.6129\n",
      "Epoch [1/5], Step [260/2116], Loss: 2.4800\n",
      "Epoch [1/5], Step [280/2116], Loss: 3.2105\n",
      "Epoch [1/5], Step [300/2116], Loss: 2.5081\n",
      "Epoch [1/5], Step [320/2116], Loss: 2.7710\n",
      "Epoch [1/5], Step [340/2116], Loss: 2.3222\n",
      "Epoch [1/5], Step [360/2116], Loss: 2.7575\n",
      "Epoch [1/5], Step [380/2116], Loss: 2.7396\n",
      "Epoch [1/5], Step [400/2116], Loss: 1.9703\n",
      "Epoch [1/5], Step [420/2116], Loss: 2.5165\n",
      "Epoch [1/5], Step [440/2116], Loss: 1.9563\n",
      "Epoch [1/5], Step [460/2116], Loss: 1.5457\n",
      "Epoch [1/5], Step [480/2116], Loss: 2.1085\n",
      "Epoch [1/5], Step [500/2116], Loss: 1.8351\n",
      "Epoch [1/5], Step [520/2116], Loss: 1.3728\n",
      "Epoch [1/5], Step [540/2116], Loss: 2.3447\n",
      "Epoch [1/5], Step [560/2116], Loss: 1.4215\n",
      "Epoch [1/5], Step [580/2116], Loss: 1.9618\n",
      "Epoch [1/5], Step [600/2116], Loss: 1.9740\n",
      "Epoch [1/5], Step [620/2116], Loss: 1.5350\n",
      "Epoch [1/5], Step [640/2116], Loss: 2.0945\n",
      "Epoch [1/5], Step [660/2116], Loss: 1.5910\n",
      "Epoch [1/5], Step [680/2116], Loss: 1.7333\n",
      "Epoch [1/5], Step [700/2116], Loss: 1.1122\n",
      "Epoch [1/5], Step [720/2116], Loss: 1.6798\n",
      "Epoch [1/5], Step [740/2116], Loss: 1.6959\n",
      "Epoch [1/5], Step [760/2116], Loss: 1.5185\n",
      "Epoch [1/5], Step [780/2116], Loss: 1.6488\n",
      "Epoch [1/5], Step [800/2116], Loss: 1.2970\n",
      "Epoch [1/5], Step [820/2116], Loss: 1.0281\n",
      "Epoch [1/5], Step [840/2116], Loss: 0.9526\n",
      "Epoch [1/5], Step [860/2116], Loss: 0.8606\n",
      "Epoch [1/5], Step [880/2116], Loss: 0.8066\n",
      "Epoch [1/5], Step [900/2116], Loss: 0.6512\n",
      "Epoch [1/5], Step [920/2116], Loss: 1.4794\n",
      "Epoch [1/5], Step [940/2116], Loss: 1.3620\n",
      "Epoch [1/5], Step [960/2116], Loss: 1.6817\n",
      "Epoch [1/5], Step [980/2116], Loss: 0.6593\n",
      "Epoch [1/5], Step [1000/2116], Loss: 1.4567\n",
      "Epoch [1/5], Step [1020/2116], Loss: 0.7370\n",
      "Epoch [1/5], Step [1040/2116], Loss: 1.1480\n",
      "Epoch [1/5], Step [1060/2116], Loss: 0.4341\n",
      "Epoch [1/5], Step [1080/2116], Loss: 0.8768\n",
      "Epoch [1/5], Step [1100/2116], Loss: 0.9058\n",
      "Epoch [1/5], Step [1120/2116], Loss: 1.5151\n",
      "Epoch [1/5], Step [1140/2116], Loss: 1.1614\n",
      "Epoch [1/5], Step [1160/2116], Loss: 0.8459\n",
      "Epoch [1/5], Step [1180/2116], Loss: 0.8297\n",
      "Epoch [1/5], Step [1200/2116], Loss: 0.3303\n",
      "Epoch [1/5], Step [1220/2116], Loss: 1.2005\n",
      "Epoch [1/5], Step [1240/2116], Loss: 0.8719\n",
      "Epoch [1/5], Step [1260/2116], Loss: 1.9265\n",
      "Epoch [1/5], Step [1280/2116], Loss: 0.7401\n",
      "Epoch [1/5], Step [1300/2116], Loss: 0.4609\n",
      "Epoch [1/5], Step [1320/2116], Loss: 1.0449\n",
      "Epoch [1/5], Step [1340/2116], Loss: 0.6386\n",
      "Epoch [1/5], Step [1360/2116], Loss: 1.1314\n",
      "Epoch [1/5], Step [1380/2116], Loss: 0.7034\n",
      "Epoch [1/5], Step [1400/2116], Loss: 0.3576\n",
      "Epoch [1/5], Step [1420/2116], Loss: 0.6207\n",
      "Epoch [1/5], Step [1440/2116], Loss: 0.8284\n",
      "Epoch [1/5], Step [1460/2116], Loss: 0.8872\n",
      "Epoch [1/5], Step [1480/2116], Loss: 0.6848\n",
      "Epoch [1/5], Step [1500/2116], Loss: 0.4611\n",
      "Epoch [1/5], Step [1520/2116], Loss: 0.3866\n",
      "Epoch [1/5], Step [1540/2116], Loss: 0.6954\n",
      "Epoch [1/5], Step [1560/2116], Loss: 0.3785\n",
      "Epoch [1/5], Step [1580/2116], Loss: 0.1480\n",
      "Epoch [1/5], Step [1600/2116], Loss: 0.4646\n",
      "Epoch [1/5], Step [1620/2116], Loss: 0.7041\n",
      "Epoch [1/5], Step [1640/2116], Loss: 0.6314\n",
      "Epoch [1/5], Step [1660/2116], Loss: 0.3595\n",
      "Epoch [1/5], Step [1680/2116], Loss: 0.2887\n",
      "Epoch [1/5], Step [1700/2116], Loss: 0.5416\n",
      "Epoch [1/5], Step [1720/2116], Loss: 0.5058\n",
      "Epoch [1/5], Step [1740/2116], Loss: 0.5385\n",
      "Epoch [1/5], Step [1760/2116], Loss: 0.7291\n",
      "Epoch [1/5], Step [1780/2116], Loss: 0.5556\n",
      "Epoch [1/5], Step [1800/2116], Loss: 0.1952\n",
      "Epoch [1/5], Step [1820/2116], Loss: 0.3379\n",
      "Epoch [1/5], Step [1840/2116], Loss: 0.4113\n",
      "Epoch [1/5], Step [1860/2116], Loss: 0.4421\n",
      "Epoch [1/5], Step [1880/2116], Loss: 0.1780\n",
      "Epoch [1/5], Step [1900/2116], Loss: 0.2671\n",
      "Epoch [1/5], Step [1920/2116], Loss: 0.6145\n",
      "Epoch [1/5], Step [1940/2116], Loss: 0.3719\n",
      "Epoch [1/5], Step [1960/2116], Loss: 0.2121\n",
      "Epoch [1/5], Step [1980/2116], Loss: 0.1915\n",
      "Epoch [1/5], Step [2000/2116], Loss: 0.2617\n",
      "Epoch [1/5], Step [2020/2116], Loss: 0.3224\n",
      "Epoch [1/5], Step [2040/2116], Loss: 0.1804\n",
      "Epoch [1/5], Step [2060/2116], Loss: 0.6121\n",
      "Epoch [1/5], Step [2080/2116], Loss: 0.2165\n",
      "Epoch [1/5], Step [2100/2116], Loss: 0.3548\n",
      "Epoch [2/5], Step [20/2116], Loss: 0.1373\n",
      "Epoch [2/5], Step [40/2116], Loss: 0.5157\n",
      "Epoch [2/5], Step [60/2116], Loss: 0.2084\n",
      "Epoch [2/5], Step [80/2116], Loss: 0.3429\n",
      "Epoch [2/5], Step [100/2116], Loss: 0.0918\n",
      "Epoch [2/5], Step [120/2116], Loss: 0.3274\n",
      "Epoch [2/5], Step [140/2116], Loss: 0.2966\n",
      "Epoch [2/5], Step [160/2116], Loss: 0.0521\n",
      "Epoch [2/5], Step [180/2116], Loss: 0.2501\n",
      "Epoch [2/5], Step [200/2116], Loss: 0.5281\n",
      "Epoch [2/5], Step [220/2116], Loss: 0.3546\n",
      "Epoch [2/5], Step [240/2116], Loss: 0.2850\n",
      "Epoch [2/5], Step [260/2116], Loss: 0.4213\n",
      "Epoch [2/5], Step [280/2116], Loss: 0.3156\n",
      "Epoch [2/5], Step [300/2116], Loss: 0.4580\n",
      "Epoch [2/5], Step [320/2116], Loss: 0.0592\n",
      "Epoch [2/5], Step [340/2116], Loss: 0.4309\n",
      "Epoch [2/5], Step [360/2116], Loss: 0.2568\n",
      "Epoch [2/5], Step [380/2116], Loss: 0.4028\n",
      "Epoch [2/5], Step [400/2116], Loss: 0.3570\n",
      "Epoch [2/5], Step [420/2116], Loss: 1.0568\n",
      "Epoch [2/5], Step [440/2116], Loss: 0.3102\n",
      "Epoch [2/5], Step [460/2116], Loss: 0.5802\n",
      "Epoch [2/5], Step [480/2116], Loss: 0.1571\n",
      "Epoch [2/5], Step [500/2116], Loss: 0.5843\n",
      "Epoch [2/5], Step [520/2116], Loss: 0.1866\n",
      "Epoch [2/5], Step [540/2116], Loss: 0.3434\n",
      "Epoch [2/5], Step [560/2116], Loss: 0.3765\n",
      "Epoch [2/5], Step [580/2116], Loss: 0.2030\n",
      "Epoch [2/5], Step [600/2116], Loss: 0.0832\n",
      "Epoch [2/5], Step [620/2116], Loss: 0.2771\n",
      "Epoch [2/5], Step [640/2116], Loss: 0.1792\n",
      "Epoch [2/5], Step [660/2116], Loss: 0.3312\n",
      "Epoch [2/5], Step [680/2116], Loss: 0.0890\n",
      "Epoch [2/5], Step [700/2116], Loss: 0.0212\n",
      "Epoch [2/5], Step [720/2116], Loss: 0.2745\n",
      "Epoch [2/5], Step [740/2116], Loss: 0.0672\n",
      "Epoch [2/5], Step [760/2116], Loss: 0.3210\n",
      "Epoch [2/5], Step [780/2116], Loss: 0.1937\n",
      "Epoch [2/5], Step [800/2116], Loss: 0.1156\n",
      "Epoch [2/5], Step [820/2116], Loss: 0.0857\n",
      "Epoch [2/5], Step [840/2116], Loss: 0.0711\n",
      "Epoch [2/5], Step [860/2116], Loss: 0.0817\n",
      "Epoch [2/5], Step [880/2116], Loss: 0.0323\n",
      "Epoch [2/5], Step [900/2116], Loss: 0.2784\n",
      "Epoch [2/5], Step [920/2116], Loss: 0.0222\n",
      "Epoch [2/5], Step [940/2116], Loss: 0.2403\n",
      "Epoch [2/5], Step [960/2116], Loss: 0.0306\n",
      "Epoch [2/5], Step [980/2116], Loss: 0.2298\n",
      "Epoch [2/5], Step [1000/2116], Loss: 0.2488\n",
      "Epoch [2/5], Step [1020/2116], Loss: 0.1542\n",
      "Epoch [2/5], Step [1040/2116], Loss: 0.5766\n",
      "Epoch [2/5], Step [1060/2116], Loss: 0.0389\n",
      "Epoch [2/5], Step [1080/2116], Loss: 0.0997\n",
      "Epoch [2/5], Step [1100/2116], Loss: 0.9789\n",
      "Epoch [2/5], Step [1120/2116], Loss: 0.0668\n",
      "Epoch [2/5], Step [1140/2116], Loss: 0.0164\n",
      "Epoch [2/5], Step [1160/2116], Loss: 0.1255\n",
      "Epoch [2/5], Step [1180/2116], Loss: 0.5891\n",
      "Epoch [2/5], Step [1200/2116], Loss: 0.0101\n",
      "Epoch [2/5], Step [1220/2116], Loss: 0.1210\n",
      "Epoch [2/5], Step [1240/2116], Loss: 0.1338\n",
      "Epoch [2/5], Step [1260/2116], Loss: 0.0233\n",
      "Epoch [2/5], Step [1280/2116], Loss: 0.2811\n",
      "Epoch [2/5], Step [1300/2116], Loss: 0.0456\n",
      "Epoch [2/5], Step [1320/2116], Loss: 0.0962\n",
      "Epoch [2/5], Step [1340/2116], Loss: 0.1705\n",
      "Epoch [2/5], Step [1360/2116], Loss: 0.0154\n",
      "Epoch [2/5], Step [1380/2116], Loss: 0.1794\n",
      "Epoch [2/5], Step [1400/2116], Loss: 0.1631\n",
      "Epoch [2/5], Step [1420/2116], Loss: 0.2731\n",
      "Epoch [2/5], Step [1440/2116], Loss: 0.2263\n",
      "Epoch [2/5], Step [1460/2116], Loss: 0.0904\n",
      "Epoch [2/5], Step [1480/2116], Loss: 0.3867\n",
      "Epoch [2/5], Step [1500/2116], Loss: 0.1822\n",
      "Epoch [2/5], Step [1520/2116], Loss: 0.3673\n",
      "Epoch [2/5], Step [1540/2116], Loss: 0.0579\n",
      "Epoch [2/5], Step [1560/2116], Loss: 0.1634\n",
      "Epoch [2/5], Step [1580/2116], Loss: 0.1096\n",
      "Epoch [2/5], Step [1600/2116], Loss: 0.0695\n",
      "Epoch [2/5], Step [1620/2116], Loss: 0.1140\n",
      "Epoch [2/5], Step [1640/2116], Loss: 0.1467\n",
      "Epoch [2/5], Step [1660/2116], Loss: 0.0349\n",
      "Epoch [2/5], Step [1680/2116], Loss: 0.0531\n",
      "Epoch [2/5], Step [1700/2116], Loss: 0.0974\n",
      "Epoch [2/5], Step [1720/2116], Loss: 0.0703\n",
      "Epoch [2/5], Step [1740/2116], Loss: 0.0643\n",
      "Epoch [2/5], Step [1760/2116], Loss: 0.0489\n",
      "Epoch [2/5], Step [1780/2116], Loss: 0.0240\n",
      "Epoch [2/5], Step [1800/2116], Loss: 0.4337\n",
      "Epoch [2/5], Step [1820/2116], Loss: 0.0224\n",
      "Epoch [2/5], Step [1840/2116], Loss: 0.0352\n",
      "Epoch [2/5], Step [1860/2116], Loss: 0.0848\n",
      "Epoch [2/5], Step [1880/2116], Loss: 0.0086\n",
      "Epoch [2/5], Step [1900/2116], Loss: 0.2408\n",
      "Epoch [2/5], Step [1920/2116], Loss: 0.1759\n",
      "Epoch [2/5], Step [1940/2116], Loss: 0.1446\n",
      "Epoch [2/5], Step [1960/2116], Loss: 0.0966\n",
      "Epoch [2/5], Step [1980/2116], Loss: 0.1080\n",
      "Epoch [2/5], Step [2000/2116], Loss: 0.2879\n",
      "Epoch [2/5], Step [2020/2116], Loss: 0.2401\n",
      "Epoch [2/5], Step [2040/2116], Loss: 0.0943\n",
      "Epoch [2/5], Step [2060/2116], Loss: 0.0331\n",
      "Epoch [2/5], Step [2080/2116], Loss: 0.0196\n",
      "Epoch [2/5], Step [2100/2116], Loss: 0.1897\n",
      "Epoch [3/5], Step [20/2116], Loss: 0.2872\n",
      "Epoch [3/5], Step [40/2116], Loss: 0.0067\n",
      "Epoch [3/5], Step [60/2116], Loss: 0.0444\n",
      "Epoch [3/5], Step [80/2116], Loss: 0.1517\n",
      "Epoch [3/5], Step [100/2116], Loss: 0.4685\n",
      "Epoch [3/5], Step [120/2116], Loss: 0.0359\n",
      "Epoch [3/5], Step [140/2116], Loss: 0.1204\n",
      "Epoch [3/5], Step [160/2116], Loss: 0.0188\n",
      "Epoch [3/5], Step [180/2116], Loss: 0.0586\n",
      "Epoch [3/5], Step [200/2116], Loss: 0.0822\n",
      "Epoch [3/5], Step [220/2116], Loss: 0.0550\n",
      "Epoch [3/5], Step [240/2116], Loss: 0.3185\n",
      "Epoch [3/5], Step [260/2116], Loss: 0.0299\n",
      "Epoch [3/5], Step [280/2116], Loss: 0.3168\n",
      "Epoch [3/5], Step [300/2116], Loss: 0.0366\n",
      "Epoch [3/5], Step [320/2116], Loss: 0.2092\n",
      "Epoch [3/5], Step [340/2116], Loss: 0.3650\n",
      "Epoch [3/5], Step [360/2116], Loss: 0.0962\n",
      "Epoch [3/5], Step [380/2116], Loss: 0.0374\n",
      "Epoch [3/5], Step [400/2116], Loss: 0.0789\n",
      "Epoch [3/5], Step [420/2116], Loss: 0.1968\n",
      "Epoch [3/5], Step [440/2116], Loss: 0.0566\n",
      "Epoch [3/5], Step [460/2116], Loss: 0.1142\n",
      "Epoch [3/5], Step [480/2116], Loss: 0.0176\n",
      "Epoch [3/5], Step [500/2116], Loss: 0.1293\n",
      "Epoch [3/5], Step [520/2116], Loss: 0.0936\n",
      "Epoch [3/5], Step [540/2116], Loss: 0.0302\n",
      "Epoch [3/5], Step [560/2116], Loss: 0.2004\n",
      "Epoch [3/5], Step [580/2116], Loss: 0.2218\n",
      "Epoch [3/5], Step [600/2116], Loss: 0.0566\n",
      "Epoch [3/5], Step [620/2116], Loss: 0.0139\n",
      "Epoch [3/5], Step [640/2116], Loss: 0.0222\n",
      "Epoch [3/5], Step [660/2116], Loss: 0.0089\n",
      "Epoch [3/5], Step [680/2116], Loss: 0.0332\n",
      "Epoch [3/5], Step [700/2116], Loss: 0.1033\n",
      "Epoch [3/5], Step [720/2116], Loss: 0.0207\n",
      "Epoch [3/5], Step [740/2116], Loss: 0.0352\n",
      "Epoch [3/5], Step [760/2116], Loss: 0.0455\n",
      "Epoch [3/5], Step [780/2116], Loss: 0.1814\n",
      "Epoch [3/5], Step [800/2116], Loss: 0.1329\n",
      "Epoch [3/5], Step [820/2116], Loss: 0.0063\n",
      "Epoch [3/5], Step [840/2116], Loss: 0.2717\n",
      "Epoch [3/5], Step [860/2116], Loss: 0.0131\n",
      "Epoch [3/5], Step [880/2116], Loss: 0.0933\n",
      "Epoch [3/5], Step [900/2116], Loss: 0.2589\n",
      "Epoch [3/5], Step [920/2116], Loss: 0.0490\n",
      "Epoch [3/5], Step [940/2116], Loss: 0.0419\n",
      "Epoch [3/5], Step [960/2116], Loss: 0.1123\n",
      "Epoch [3/5], Step [980/2116], Loss: 0.2117\n",
      "Epoch [3/5], Step [1000/2116], Loss: 0.2397\n",
      "Epoch [3/5], Step [1020/2116], Loss: 0.1601\n",
      "Epoch [3/5], Step [1040/2116], Loss: 0.0759\n",
      "Epoch [3/5], Step [1060/2116], Loss: 0.2351\n",
      "Epoch [3/5], Step [1080/2116], Loss: 0.0641\n",
      "Epoch [3/5], Step [1100/2116], Loss: 0.0015\n",
      "Epoch [3/5], Step [1120/2116], Loss: 0.1143\n",
      "Epoch [3/5], Step [1140/2116], Loss: 0.1996\n",
      "Epoch [3/5], Step [1160/2116], Loss: 0.0094\n",
      "Epoch [3/5], Step [1180/2116], Loss: 0.0018\n",
      "Epoch [3/5], Step [1200/2116], Loss: 0.1864\n",
      "Epoch [3/5], Step [1220/2116], Loss: 0.1629\n",
      "Epoch [3/5], Step [1240/2116], Loss: 0.0524\n",
      "Epoch [3/5], Step [1260/2116], Loss: 0.0149\n",
      "Epoch [3/5], Step [1280/2116], Loss: 0.0303\n",
      "Epoch [3/5], Step [1300/2116], Loss: 0.0041\n",
      "Epoch [3/5], Step [1320/2116], Loss: 0.0922\n",
      "Epoch [3/5], Step [1340/2116], Loss: 0.0577\n",
      "Epoch [3/5], Step [1360/2116], Loss: 0.0050\n",
      "Epoch [3/5], Step [1380/2116], Loss: 0.1607\n",
      "Epoch [3/5], Step [1400/2116], Loss: 0.1074\n",
      "Epoch [3/5], Step [1420/2116], Loss: 0.0835\n",
      "Epoch [3/5], Step [1440/2116], Loss: 0.0578\n",
      "Epoch [3/5], Step [1460/2116], Loss: 0.0248\n",
      "Epoch [3/5], Step [1480/2116], Loss: 0.0690\n",
      "Epoch [3/5], Step [1500/2116], Loss: 1.0170\n",
      "Epoch [3/5], Step [1520/2116], Loss: 0.1080\n",
      "Epoch [3/5], Step [1540/2116], Loss: 0.1261\n",
      "Epoch [3/5], Step [1560/2116], Loss: 0.1025\n",
      "Epoch [3/5], Step [1580/2116], Loss: 0.0467\n",
      "Epoch [3/5], Step [1600/2116], Loss: 0.0907\n",
      "Epoch [3/5], Step [1620/2116], Loss: 0.5551\n",
      "Epoch [3/5], Step [1640/2116], Loss: 0.0484\n",
      "Epoch [3/5], Step [1660/2116], Loss: 0.2119\n",
      "Epoch [3/5], Step [1680/2116], Loss: 0.1082\n",
      "Epoch [3/5], Step [1700/2116], Loss: 0.1320\n",
      "Epoch [3/5], Step [1720/2116], Loss: 0.0056\n",
      "Epoch [3/5], Step [1740/2116], Loss: 0.0112\n",
      "Epoch [3/5], Step [1760/2116], Loss: 0.0202\n",
      "Epoch [3/5], Step [1780/2116], Loss: 0.0047\n",
      "Epoch [3/5], Step [1800/2116], Loss: 0.0130\n",
      "Epoch [3/5], Step [1820/2116], Loss: 0.1577\n",
      "Epoch [3/5], Step [1840/2116], Loss: 0.0175\n",
      "Epoch [3/5], Step [1860/2116], Loss: 0.0685\n",
      "Epoch [3/5], Step [1880/2116], Loss: 0.0213\n",
      "Epoch [3/5], Step [1900/2116], Loss: 0.0372\n",
      "Epoch [3/5], Step [1920/2116], Loss: 0.0108\n",
      "Epoch [3/5], Step [1940/2116], Loss: 0.0039\n",
      "Epoch [3/5], Step [1960/2116], Loss: 0.0786\n",
      "Epoch [3/5], Step [1980/2116], Loss: 0.0209\n",
      "Epoch [3/5], Step [2000/2116], Loss: 0.0111\n",
      "Epoch [3/5], Step [2020/2116], Loss: 0.0155\n",
      "Epoch [3/5], Step [2040/2116], Loss: 0.0153\n",
      "Epoch [3/5], Step [2060/2116], Loss: 0.0353\n",
      "Epoch [3/5], Step [2080/2116], Loss: 0.0082\n",
      "Epoch [3/5], Step [2100/2116], Loss: 0.0262\n",
      "Epoch [4/5], Step [20/2116], Loss: 0.1583\n",
      "Epoch [4/5], Step [40/2116], Loss: 0.0971\n",
      "Epoch [4/5], Step [60/2116], Loss: 0.0034\n",
      "Epoch [4/5], Step [80/2116], Loss: 0.0438\n",
      "Epoch [4/5], Step [100/2116], Loss: 0.0170\n",
      "Epoch [4/5], Step [120/2116], Loss: 0.0140\n",
      "Epoch [4/5], Step [140/2116], Loss: 0.0494\n",
      "Epoch [4/5], Step [160/2116], Loss: 0.0287\n",
      "Epoch [4/5], Step [180/2116], Loss: 0.1370\n",
      "Epoch [4/5], Step [200/2116], Loss: 0.4151\n",
      "Epoch [4/5], Step [220/2116], Loss: 0.0033\n",
      "Epoch [4/5], Step [240/2116], Loss: 0.1218\n",
      "Epoch [4/5], Step [260/2116], Loss: 0.1543\n",
      "Epoch [4/5], Step [280/2116], Loss: 0.0700\n",
      "Epoch [4/5], Step [300/2116], Loss: 0.0011\n",
      "Epoch [4/5], Step [320/2116], Loss: 0.0189\n",
      "Epoch [4/5], Step [340/2116], Loss: 0.0030\n",
      "Epoch [4/5], Step [360/2116], Loss: 0.0151\n",
      "Epoch [4/5], Step [380/2116], Loss: 0.2199\n",
      "Epoch [4/5], Step [400/2116], Loss: 0.0057\n",
      "Epoch [4/5], Step [420/2116], Loss: 0.1729\n",
      "Epoch [4/5], Step [440/2116], Loss: 0.0015\n",
      "Epoch [4/5], Step [460/2116], Loss: 0.0085\n",
      "Epoch [4/5], Step [480/2116], Loss: 0.0082\n",
      "Epoch [4/5], Step [500/2116], Loss: 0.0149\n",
      "Epoch [4/5], Step [520/2116], Loss: 0.0158\n",
      "Epoch [4/5], Step [540/2116], Loss: 0.0206\n",
      "Epoch [4/5], Step [560/2116], Loss: 0.0132\n",
      "Epoch [4/5], Step [580/2116], Loss: 0.0063\n",
      "Epoch [4/5], Step [600/2116], Loss: 0.0161\n",
      "Epoch [4/5], Step [620/2116], Loss: 0.0420\n",
      "Epoch [4/5], Step [640/2116], Loss: 0.0068\n",
      "Epoch [4/5], Step [660/2116], Loss: 0.0258\n",
      "Epoch [4/5], Step [680/2116], Loss: 0.0156\n",
      "Epoch [4/5], Step [700/2116], Loss: 0.0053\n",
      "Epoch [4/5], Step [720/2116], Loss: 0.0288\n",
      "Epoch [4/5], Step [740/2116], Loss: 0.1125\n",
      "Epoch [4/5], Step [760/2116], Loss: 0.0569\n",
      "Epoch [4/5], Step [780/2116], Loss: 0.0059\n",
      "Epoch [4/5], Step [800/2116], Loss: 0.0623\n",
      "Epoch [4/5], Step [820/2116], Loss: 0.0189\n",
      "Epoch [4/5], Step [840/2116], Loss: 0.0027\n",
      "Epoch [4/5], Step [860/2116], Loss: 0.0075\n",
      "Epoch [4/5], Step [880/2116], Loss: 0.0803\n",
      "Epoch [4/5], Step [900/2116], Loss: 0.0067\n",
      "Epoch [4/5], Step [920/2116], Loss: 0.0731\n",
      "Epoch [4/5], Step [940/2116], Loss: 0.0857\n",
      "Epoch [4/5], Step [960/2116], Loss: 0.0441\n",
      "Epoch [4/5], Step [980/2116], Loss: 0.0212\n",
      "Epoch [4/5], Step [1000/2116], Loss: 0.0775\n",
      "Epoch [4/5], Step [1020/2116], Loss: 0.0834\n",
      "Epoch [4/5], Step [1040/2116], Loss: 0.0148\n",
      "Epoch [4/5], Step [1060/2116], Loss: 0.0328\n",
      "Epoch [4/5], Step [1080/2116], Loss: 0.0319\n",
      "Epoch [4/5], Step [1100/2116], Loss: 0.1121\n",
      "Epoch [4/5], Step [1120/2116], Loss: 0.0509\n",
      "Epoch [4/5], Step [1140/2116], Loss: 0.0485\n",
      "Epoch [4/5], Step [1160/2116], Loss: 0.0009\n",
      "Epoch [4/5], Step [1180/2116], Loss: 0.0157\n",
      "Epoch [4/5], Step [1200/2116], Loss: 0.0242\n",
      "Epoch [4/5], Step [1220/2116], Loss: 0.0011\n",
      "Epoch [4/5], Step [1240/2116], Loss: 0.0190\n",
      "Epoch [4/5], Step [1260/2116], Loss: 0.0080\n",
      "Epoch [4/5], Step [1280/2116], Loss: 0.0548\n",
      "Epoch [4/5], Step [1300/2116], Loss: 0.2048\n",
      "Epoch [4/5], Step [1320/2116], Loss: 0.1658\n",
      "Epoch [4/5], Step [1340/2116], Loss: 0.0077\n",
      "Epoch [4/5], Step [1360/2116], Loss: 0.0103\n",
      "Epoch [4/5], Step [1380/2116], Loss: 0.0243\n",
      "Epoch [4/5], Step [1400/2116], Loss: 0.2058\n",
      "Epoch [4/5], Step [1420/2116], Loss: 0.0020\n",
      "Epoch [4/5], Step [1440/2116], Loss: 0.0951\n",
      "Epoch [4/5], Step [1460/2116], Loss: 0.0011\n",
      "Epoch [4/5], Step [1480/2116], Loss: 0.0050\n",
      "Epoch [4/5], Step [1500/2116], Loss: 0.0006\n",
      "Epoch [4/5], Step [1520/2116], Loss: 0.0077\n",
      "Epoch [4/5], Step [1540/2116], Loss: 0.0036\n",
      "Epoch [4/5], Step [1560/2116], Loss: 0.0007\n",
      "Epoch [4/5], Step [1580/2116], Loss: 0.0026\n",
      "Epoch [4/5], Step [1600/2116], Loss: 0.0007\n",
      "Epoch [4/5], Step [1620/2116], Loss: 0.0036\n",
      "Epoch [4/5], Step [1640/2116], Loss: 0.0056\n",
      "Epoch [4/5], Step [1660/2116], Loss: 0.0219\n",
      "Epoch [4/5], Step [1680/2116], Loss: 0.0438\n",
      "Epoch [4/5], Step [1700/2116], Loss: 0.0058\n",
      "Epoch [4/5], Step [1720/2116], Loss: 0.0060\n",
      "Epoch [4/5], Step [1740/2116], Loss: 0.0056\n",
      "Epoch [4/5], Step [1760/2116], Loss: 0.0154\n",
      "Epoch [4/5], Step [1780/2116], Loss: 0.0017\n",
      "Epoch [4/5], Step [1800/2116], Loss: 0.0017\n",
      "Epoch [4/5], Step [1820/2116], Loss: 0.0638\n",
      "Epoch [4/5], Step [1840/2116], Loss: 0.0030\n",
      "Epoch [4/5], Step [1860/2116], Loss: 0.0373\n",
      "Epoch [4/5], Step [1880/2116], Loss: 0.0052\n",
      "Epoch [4/5], Step [1900/2116], Loss: 0.0005\n",
      "Epoch [4/5], Step [1920/2116], Loss: 0.1254\n",
      "Epoch [4/5], Step [1940/2116], Loss: 0.0009\n",
      "Epoch [4/5], Step [1960/2116], Loss: 0.0050\n",
      "Epoch [4/5], Step [1980/2116], Loss: 0.0024\n",
      "Epoch [4/5], Step [2000/2116], Loss: 0.0029\n",
      "Epoch [4/5], Step [2020/2116], Loss: 0.0180\n",
      "Epoch [4/5], Step [2040/2116], Loss: 0.0635\n",
      "Epoch [4/5], Step [2060/2116], Loss: 0.0109\n",
      "Epoch [4/5], Step [2080/2116], Loss: 0.0948\n",
      "Epoch [4/5], Step [2100/2116], Loss: 0.0011\n",
      "Epoch [5/5], Step [20/2116], Loss: 0.0469\n",
      "Epoch [5/5], Step [40/2116], Loss: 0.0439\n",
      "Epoch [5/5], Step [60/2116], Loss: 0.0017\n",
      "Epoch [5/5], Step [80/2116], Loss: 0.0399\n",
      "Epoch [5/5], Step [100/2116], Loss: 0.0690\n",
      "Epoch [5/5], Step [120/2116], Loss: 0.0163\n",
      "Epoch [5/5], Step [140/2116], Loss: 0.1723\n",
      "Epoch [5/5], Step [160/2116], Loss: 0.3161\n",
      "Epoch [5/5], Step [180/2116], Loss: 0.0888\n",
      "Epoch [5/5], Step [200/2116], Loss: 0.0358\n",
      "Epoch [5/5], Step [220/2116], Loss: 0.0004\n",
      "Epoch [5/5], Step [240/2116], Loss: 0.0002\n",
      "Epoch [5/5], Step [260/2116], Loss: 0.0049\n",
      "Epoch [5/5], Step [280/2116], Loss: 0.0133\n",
      "Epoch [5/5], Step [300/2116], Loss: 0.0020\n",
      "Epoch [5/5], Step [320/2116], Loss: 0.0068\n",
      "Epoch [5/5], Step [340/2116], Loss: 0.0026\n",
      "Epoch [5/5], Step [360/2116], Loss: 0.0096\n",
      "Epoch [5/5], Step [380/2116], Loss: 0.0027\n",
      "Epoch [5/5], Step [400/2116], Loss: 0.0956\n",
      "Epoch [5/5], Step [420/2116], Loss: 0.0103\n",
      "Epoch [5/5], Step [440/2116], Loss: 0.0008\n",
      "Epoch [5/5], Step [460/2116], Loss: 0.0085\n",
      "Epoch [5/5], Step [480/2116], Loss: 0.0004\n",
      "Epoch [5/5], Step [500/2116], Loss: 0.0027\n",
      "Epoch [5/5], Step [520/2116], Loss: 0.0006\n",
      "Epoch [5/5], Step [540/2116], Loss: 0.2154\n",
      "Epoch [5/5], Step [560/2116], Loss: 0.0007\n",
      "Epoch [5/5], Step [580/2116], Loss: 0.0052\n",
      "Epoch [5/5], Step [600/2116], Loss: 0.0024\n",
      "Epoch [5/5], Step [620/2116], Loss: 0.0014\n",
      "Epoch [5/5], Step [640/2116], Loss: 0.0814\n",
      "Epoch [5/5], Step [660/2116], Loss: 0.0135\n",
      "Epoch [5/5], Step [680/2116], Loss: 0.0113\n",
      "Epoch [5/5], Step [700/2116], Loss: 0.0009\n",
      "Epoch [5/5], Step [720/2116], Loss: 0.0216\n",
      "Epoch [5/5], Step [740/2116], Loss: 0.0234\n",
      "Epoch [5/5], Step [760/2116], Loss: 0.0099\n",
      "Epoch [5/5], Step [780/2116], Loss: 0.0582\n",
      "Epoch [5/5], Step [800/2116], Loss: 0.0212\n",
      "Epoch [5/5], Step [820/2116], Loss: 0.0064\n",
      "Epoch [5/5], Step [840/2116], Loss: 0.0062\n",
      "Epoch [5/5], Step [860/2116], Loss: 0.0010\n",
      "Epoch [5/5], Step [880/2116], Loss: 0.0003\n",
      "Epoch [5/5], Step [900/2116], Loss: 0.0018\n",
      "Epoch [5/5], Step [920/2116], Loss: 0.0026\n",
      "Epoch [5/5], Step [940/2116], Loss: 0.0007\n",
      "Epoch [5/5], Step [960/2116], Loss: 0.0022\n",
      "Epoch [5/5], Step [980/2116], Loss: 0.0026\n",
      "Epoch [5/5], Step [1000/2116], Loss: 0.0029\n",
      "Epoch [5/5], Step [1020/2116], Loss: 0.1436\n",
      "Epoch [5/5], Step [1040/2116], Loss: 0.0311\n",
      "Epoch [5/5], Step [1060/2116], Loss: 0.0108\n",
      "Epoch [5/5], Step [1080/2116], Loss: 0.0102\n",
      "Epoch [5/5], Step [1100/2116], Loss: 0.1383\n",
      "Epoch [5/5], Step [1120/2116], Loss: 0.2934\n",
      "Epoch [5/5], Step [1140/2116], Loss: 0.0240\n",
      "Epoch [5/5], Step [1160/2116], Loss: 0.0124\n",
      "Epoch [5/5], Step [1180/2116], Loss: 0.0002\n",
      "Epoch [5/5], Step [1200/2116], Loss: 0.0006\n",
      "Epoch [5/5], Step [1220/2116], Loss: 0.0028\n",
      "Epoch [5/5], Step [1240/2116], Loss: 0.0163\n",
      "Epoch [5/5], Step [1260/2116], Loss: 0.0158\n",
      "Epoch [5/5], Step [1280/2116], Loss: 0.0600\n",
      "Epoch [5/5], Step [1300/2116], Loss: 0.0077\n",
      "Epoch [5/5], Step [1320/2116], Loss: 0.0035\n",
      "Epoch [5/5], Step [1340/2116], Loss: 0.0001\n",
      "Epoch [5/5], Step [1360/2116], Loss: 0.0081\n",
      "Epoch [5/5], Step [1380/2116], Loss: 0.0002\n",
      "Epoch [5/5], Step [1400/2116], Loss: 0.0276\n",
      "Epoch [5/5], Step [1420/2116], Loss: 0.0034\n",
      "Epoch [5/5], Step [1440/2116], Loss: 0.0008\n",
      "Epoch [5/5], Step [1460/2116], Loss: 0.0274\n",
      "Epoch [5/5], Step [1480/2116], Loss: 0.0054\n",
      "Epoch [5/5], Step [1500/2116], Loss: 0.0054\n",
      "Epoch [5/5], Step [1520/2116], Loss: 0.0496\n",
      "Epoch [5/5], Step [1540/2116], Loss: 0.0121\n",
      "Epoch [5/5], Step [1560/2116], Loss: 0.0031\n",
      "Epoch [5/5], Step [1580/2116], Loss: 0.0003\n",
      "Epoch [5/5], Step [1600/2116], Loss: 0.0057\n",
      "Epoch [5/5], Step [1620/2116], Loss: 0.0104\n",
      "Epoch [5/5], Step [1640/2116], Loss: 0.0042\n",
      "Epoch [5/5], Step [1660/2116], Loss: 0.0108\n",
      "Epoch [5/5], Step [1680/2116], Loss: 0.0015\n",
      "Epoch [5/5], Step [1700/2116], Loss: 0.0033\n",
      "Epoch [5/5], Step [1720/2116], Loss: 0.0041\n",
      "Epoch [5/5], Step [1740/2116], Loss: 0.0017\n",
      "Epoch [5/5], Step [1760/2116], Loss: 0.0224\n",
      "Epoch [5/5], Step [1780/2116], Loss: 0.1712\n",
      "Epoch [5/5], Step [1800/2116], Loss: 0.1221\n",
      "Epoch [5/5], Step [1820/2116], Loss: 0.1177\n",
      "Epoch [5/5], Step [1840/2116], Loss: 0.0027\n",
      "Epoch [5/5], Step [1860/2116], Loss: 0.0012\n",
      "Epoch [5/5], Step [1880/2116], Loss: 0.0083\n",
      "Epoch [5/5], Step [1900/2116], Loss: 0.0079\n",
      "Epoch [5/5], Step [1920/2116], Loss: 0.2167\n",
      "Epoch [5/5], Step [1940/2116], Loss: 0.0394\n",
      "Epoch [5/5], Step [1960/2116], Loss: 0.0013\n",
      "Epoch [5/5], Step [1980/2116], Loss: 0.0120\n",
      "Epoch [5/5], Step [2000/2116], Loss: 0.0924\n",
      "Epoch [5/5], Step [2020/2116], Loss: 0.0006\n",
      "Epoch [5/5], Step [2040/2116], Loss: 0.0087\n",
      "Epoch [5/5], Step [2060/2116], Loss: 0.0390\n",
      "Epoch [5/5], Step [2080/2116], Loss: 0.0030\n",
      "Epoch [5/5], Step [2100/2116], Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "total_step = len(trainloader)\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T13:27:43.229251Z",
     "iopub.status.busy": "2021-12-21T13:27:43.228443Z",
     "iopub.status.idle": "2021-12-21T13:27:44.550103Z",
     "shell.execute_reply": "2021-12-21T13:27:44.549374Z",
     "shell.execute_reply.started": "2021-12-21T13:27:43.229205Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-21T13:28:56.756843Z",
     "iopub.status.busy": "2021-12-21T13:28:56.756390Z",
     "iopub.status.idle": "2021-12-21T13:32:12.641766Z",
     "shell.execute_reply": "2021-12-21T13:32:12.641024Z",
     "shell.execute_reply.started": "2021-12-21T13:28:56.756805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 97.34220733427362%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {}%'\\\n",
    "          .format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
